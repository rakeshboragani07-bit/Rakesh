{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199b4071",
   "metadata": {},
   "source": [
    "# CS5760 — Natural Language Processing  \n",
    "**Homework 1 — Answers & Code**  \n",
    "\n",
    "**Student:** _(fill your name here)_  \n",
    "**Semester:** Fall 2025  \n",
    "**Course:** University of Central Missouri — CS5760\n",
    "\n",
    "> This notebook contains solutions and runnable code for:\n",
    "> - Q1: Regular Expressions\n",
    "> - Q2: Tokenization (naïve vs tool), MWEs, Reflection\n",
    "> - Q3: Byte Pair Encoding (manual and code)\n",
    "> - Q4: Minimum Edit Distance (two cost models + DP table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1b40b",
   "metadata": {},
   "source": [
    "## Q1. Regex\n",
    "\n",
    "We provide patterns and quick demonstrations. Adjust test strings as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6c34c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Q1 DEMO RESULTS ===\n",
      "ZIP matches: ['12345', '12345-6789', '12345 6789']\n",
      "Non-capital-start words: ['examples', 'bad', 'apple', \"don't\", 'state-of-the-art', 'banana', 'bad', 'variants', 'email', 'e-mail', 'e', 'mail'] ... (total: 35 )\n",
      "Numbers: ['123', '45', '123', '45', '-678', '9', '123', '45', '678', '9', '123', '4', '123', '45', '+1', '-2', '1,234', '12,345.67', '-3.5e-2', '+10E+3', '007', '1', '2', '3']\n",
      "Email variants: ['Email', 'email', 'e-mail', 'e–mail', 'e mail', 'E-Mail']\n",
      "Interjection 'go+' matches: ['go', 'goo', 'gooo', 'go', 'gooo', 'gooo']\n",
      "Lines ending like a question (with closing quotes/brackets): ['Questions: He said \"Really?\"  Did you ask “why?”)   End like this?  Or maybe?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "test_text = \"\"\"\n",
    "ZIP examples: 12345, 12345-6789, 12345 6789, bad: 1234, embeddedX12345Y.\n",
    "Words: apple, Apple, don't, state-of-the-art, banana.\n",
    "Numbers: +1, -2, 1,234, 12,345.67, -3.5e-2, +10E+3, 007, 1,2,3 (bad).\n",
    "Email variants: email, e-mail, e–mail, e mail, E-Mail, NOT: remail, emailed (should not match whole-token).\n",
    "Interjection: go, goo, gooo!, go?, g, ogoo, gooo, gooo.\n",
    "Questions: He said \"Really?\"  Did you ask “why?”)   End like this?  Or maybe?\n",
    "\"\"\".strip()\n",
    "\n",
    "# (a) U.S. ZIP codes: 12345 OR 12345-6789 OR 12345 6789, only as whole tokens (no embedding)\n",
    "pattern_zip = re.compile(r\"\\b\\d{5}(?:[-\\s]\\d{4})?\\b\")\n",
    "zips = pattern_zip.findall(test_text)\n",
    "\n",
    "# (b) Words that do not start with a capital letter (allow internal apostrophes/hyphens)\n",
    "pattern_noncap = re.compile(r\"\\b(?![A-Z])[A-Za-z]+(?:['-][A-Za-z]+)*\\b\")\n",
    "noncap_words = pattern_noncap.findall(test_text)\n",
    "\n",
    "# (c) Numbers with optional sign, thousands separators, decimals, and scientific notation\n",
    "pattern_numbers = re.compile(r\"[+-]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d+)?(?:[eE][+-]?\\d+)?\")\n",
    "numbers = pattern_numbers.findall(test_text)\n",
    "\n",
    "# (d) Variants of “email”: email, e-mail, e–mail (en-dash), e mail (case-insensitive), word-boundary\n",
    "pattern_email = re.compile(r\"(?i)\\be(?:[-–\\s]?mail)\\b\")\n",
    "emails = pattern_email.findall(test_text)\n",
    "\n",
    "# (e) Interjection: go, goo, gooo… as a word, optional trailing punctuation ! . , ?\n",
    "pattern_go = re.compile(r\"\\bgo+[\\!\\.\\,\\?]?\\b\", re.IGNORECASE)\n",
    "go_matches = pattern_go.findall(test_text)\n",
    "\n",
    "# (f) Lines that end with a question mark possibly followed only by closing quotes/brackets and spaces\n",
    "# We'll test per-line:\n",
    "pattern_question_end = re.compile(r\"\\?[)\\\"”’\\]\\s]*$\")\n",
    "question_lines = []\n",
    "for line in test_text.splitlines():\n",
    "    if pattern_question_end.search(line):\n",
    "        question_lines.append(line)\n",
    "\n",
    "print(\"=== Q1 DEMO RESULTS ===\")\n",
    "print(\"ZIP matches:\", zips)\n",
    "print(\"Non-capital-start words:\", noncap_words[:12], \"... (total:\", len(noncap_words), \")\")\n",
    "print(\"Numbers:\", numbers)\n",
    "print(\"Email variants:\", emails)\n",
    "print(\"Interjection 'go+' matches:\", go_matches)\n",
    "print(\"Lines ending like a question (with closing quotes/brackets):\", question_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d311a",
   "metadata": {},
   "source": [
    "## Q2. Programming: Tokenization (Telugu example)\n",
    "\n",
    "We use a short 3–4 sentence paragraph in Telugu, then show:\n",
    "1) naïve space-based tokenization,  \n",
    "2) a manual correction (punctuation separated; keep morphemes intact for clarity),  \n",
    "3) a simple regex-based tokenizer as the \"tool\" output for comparison, and  \n",
    "4) MWEs and a brief reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "paragraph = \"రాము పుస్తకం చదువుతున్నాడు. నేను ఆపిల్ తిన్నాను! ఇది చాలా మంచి కథ.\"\n",
    "\n",
    "# (1) Naïve space-based tokenization\n",
    "naive_tokens = paragraph.split()\n",
    "\n",
    "# (2) Manual correction: separate trailing punctuation (.!?), keep words otherwise intact\n",
    "def manual_tokenize(text):\n",
    "    tokens = []\n",
    "    for tok in text.split():\n",
    "        m = re.match(r\"^(.*?)([\\.!\\?,;:\\\"'”’)\\]]+)$\", tok)\n",
    "        if m:\n",
    "            core, punct = m.group(1), m.group(2)\n",
    "            if core:\n",
    "                tokens.append(core)\n",
    "            tokens.extend(list(punct))  # keep each punctuation as its own token\n",
    "        else:\n",
    "            tokens.append(tok)\n",
    "    return tokens\n",
    "\n",
    "manual_tokens = manual_tokenize(paragraph)\n",
    "\n",
    "# (3) \"Tool\" tokenizer: regex-based (Unicode-aware): words or single punctuation marks\n",
    "tool_tokens = re.findall(r\"\\w+|[^\\w\\s]\", paragraph, flags=re.UNICODE)\n",
    "\n",
    "# Compare differences\n",
    "def compare_lists(a, b):\n",
    "    max_len = max(len(a), len(b))\n",
    "    rows = []\n",
    "    for i in range(max_len):\n",
    "        rows.append((i, a[i] if i < len(a) else \"\", b[i] if i < len(b) else \"\"))\n",
    "    return rows\n",
    "\n",
    "print(\"=== Q2 DEMO RESULTS ===\")\n",
    "print(\"Paragraph:\", paragraph)\n",
    "print(\"\\nNaïve tokens:\")\n",
    "print(naive_tokens)\n",
    "print(\"\\nManual tokens:\")\n",
    "print(manual_tokens)\n",
    "print(\"\\nTool tokens (regex-based):\")\n",
    "print(tool_tokens)\n",
    "\n",
    "rows_naive_manual = compare_lists(naive_tokens, manual_tokens)\n",
    "rows_manual_tool = compare_lists(manual_tokens, tool_tokens)\n",
    "\n",
    "# Show first 20 comparisons for brevity\n",
    "print(\"\\nNaïve vs Manual (first 20):\")\n",
    "for r in rows_naive_manual[:20]:\n",
    "    print(r)\n",
    "\n",
    "print(\"\\nManual vs Tool (first 20):\")\n",
    "for r in rows_manual_tool[:20]:\n",
    "    print(r)\n",
    "\n",
    "# (4) MWEs (examples) — reasons they should be treated as a single unit\n",
    "mwes = [\n",
    "    (\"హైదరాబాద్ నగరం\", \"Place name; treated as one semantic unit.\"),\n",
    "    (\"రైల్వే స్టేషన్\", \"Compound noun frequently used as a fixed phrase.\"),\n",
    "    (\"చేతులు కాలేయడం\", \"Idiom; literal tokenization obscures idiomatic meaning.\")\n",
    "]\n",
    "\n",
    "print(\"\\nMWEs (example rationale):\")\n",
    "for m, why in mwes:\n",
    "    print(f\"- {m}: {why}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d7e26",
   "metadata": {},
   "source": [
    "### Q2 — Reflection (5–6 sentences)\n",
    "Tokenization in Telugu is challenging due to agglutinative morphology: tense, aspect, person, case, and honorific markers attach directly to stems (e.g., *చదువు → చదువుతున్నాడు*). Naïve space splitting leaves punctuation glued to words and ignores suffix boundaries that carry meaning. Compared with English, where spaces correspond fairly well to tokens, Telugu requires either rule-based morphological heuristics or subword methods to capture meaning-bearing units. Multiword expressions like place names and idioms are also problematic because they are syntactically multi-token but semantically single units. Simple Unicode-aware regex tokenizers improve punctuation handling but still miss morpheme boundaries. For robust downstream NLP, subword models or morphological analyzers are preferable to raw whitespace tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a740b8",
   "metadata": {},
   "source": [
    "## Q3. Manual BPE and Mini-BPE\n",
    "\n",
    "We use the toy corpus (with end-of-word `_`) and perform three manual merges, then a small BPE learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc045bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Toy corpus words (without underscores), as given\n",
    "words = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\".split()\n",
    "\n",
    "def add_eow(words):\n",
    "    # Represent each word as characters plus end-of-word marker '_'\n",
    "    return [' '.join(list(w)) + ' _' for w in words]\n",
    "\n",
    "corpus = add_eow(words)\n",
    "\n",
    "def get_pair_stats(corpus):\n",
    "    counts = Counter()\n",
    "    for w in corpus:\n",
    "        syms = w.split()\n",
    "        for i in range(len(syms)-1):\n",
    "            pair = (syms[i], syms[i+1])\n",
    "            counts[pair] += 1\n",
    "    return counts\n",
    "\n",
    "def do_merge(corpus, pair):\n",
    "    a, b = pair\n",
    "    pattern = f\"{a} {b}\"\n",
    "    repl = f\"{a}{b}\"\n",
    "    new_corpus = [w.replace(pattern, repl) for w in corpus]\n",
    "    return new_corpus, repl\n",
    "\n",
    "print(\"=== Q3.1 Manual BPE (First 3 merges) ===\")\n",
    "for step in range(1, 4):\n",
    "    stats = get_pair_stats(corpus)\n",
    "    best = max(stats, key=stats.get)\n",
    "    corpus, new_token = do_merge(corpus, best)\n",
    "    print(f\"Step {step}: merge {best} -> '{new_token}'\")\n",
    "    print(\"Sample lines:\", corpus[:4], \"\\n\")\n",
    "\n",
    "# After 3 merges, show current vocabulary\n",
    "vocab = Counter()\n",
    "for w in corpus:\n",
    "    for s in w.split():\n",
    "        vocab[s] += 1\n",
    "\n",
    "print(\"Current vocabulary size:\", len(vocab))\n",
    "print(\"Some tokens:\", list(vocab.keys())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "def learn_bpe(words, num_merges=10):\n",
    "    corpus = [' '.join(list(w)) + ' _' for w in words]\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        # count pairs\n",
    "        counts = Counter()\n",
    "        for w in corpus:\n",
    "            syms = w.split()\n",
    "            for j in range(len(syms)-1):\n",
    "                counts[(syms[j], syms[j+1])] += 1\n",
    "        if not counts:\n",
    "            break\n",
    "        best = max(counts, key=counts.get)\n",
    "        merges.append(best)\n",
    "        # apply merge\n",
    "        a, b = best\n",
    "        pattern = f\"{a} {b}\"\n",
    "        repl = f\"{a}{b}\"\n",
    "        corpus = [w.replace(pattern, repl) for w in corpus]\n",
    "        print(f\"Merge {i+1}: {best}, top count={counts[best]}\")\n",
    "    return merges\n",
    "\n",
    "def apply_bpe(word, merges):\n",
    "    # segment a word using learned merges (left-to-right, greedy via replacements on the space-joined representation)\n",
    "    s = ' '.join(list(word)) + ' _'\n",
    "    for a, b in merges:\n",
    "        s = s.replace(f\"{a} {b}\", f\"{a}{b}\")\n",
    "    return s.split()\n",
    "\n",
    "toy_words = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\".split()\n",
    "merges = learn_bpe(toy_words, num_merges=12)\n",
    "\n",
    "print(\"\\nLearned merges:\", merges)\n",
    "print(\"Vocabulary size (approx tokens):\", len(set(t for w in toy_words for t in apply_bpe(w, merges))))\n",
    "\n",
    "# Segment requested words\n",
    "for w in [\"new\", \"newer\", \"lowest\", \"widest\", \"newestest\"]:\n",
    "    print(w, \"→\", apply_bpe(w, merges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30050aa7",
   "metadata": {},
   "source": [
    "### Q3 — Reflection (5–6 sentences)\n",
    "Subword tokens mitigate the OOV problem by decomposing unseen words into known pieces, allowing the model to handle novel compositions like *newestest* with segments such as `new`, `est`, and `_`. In the toy English corpus, some merges align with meaningful morphemes (e.g., `er_` as the comparative/agentive suffix), while others are purely frequency-driven. As merges increase, frequent stems like `new` and affixes like `er_` or `est_` tend to emerge. However, subword splits may not always respect true morphological boundaries, creating unnatural fragments. For agglutinative languages, many suffixes can be learned as subwords, improving generalization. The balance between vocabulary size and granularity influences both efficiency and linguistic fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8a00b",
   "metadata": {},
   "source": [
    "### Q3.3 — BPE on your own paragraph (English example for reproducibility)\n",
    "\n",
    "We train on a small English paragraph (so this notebook runs offline with consistent output), learn 30 merges, show top 5 merges and 5 longest subword tokens, and segment 5 words (including a rare and an inflected form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "paragraph_en = (\n",
    "    \"Ramu is reading a small story. \"\n",
    "    \"This story is about reading habits and readers. \"\n",
    "    \"Reading stories helps readers improve skills. \"\n",
    "    \"Some readers read slower, others read faster.\"\n",
    ")\n",
    "\n",
    "def train_bpe_from_text(text, num_merges=30):\n",
    "    words = []\n",
    "    for w in re.findall(r\"\\w+\", text.lower(), flags=re.UNICODE):\n",
    "        words.append(w)\n",
    "    corpus = [' '.join(list(w)) + ' _' for w in words]\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        counts = Counter()\n",
    "        for w in corpus:\n",
    "            syms = w.split()\n",
    "            for i in range(len(syms)-1):\n",
    "                counts[(syms[i], syms[i+1])] += 1\n",
    "        if not counts:\n",
    "            break\n",
    "        best = max(counts, key=counts.get)\n",
    "        merges.append(best)\n",
    "        a, b = best\n",
    "        pattern = f\"{a} {b}\"\n",
    "        repl = f\"{a}{b}\"\n",
    "        corpus = [w.replace(pattern, repl) for w in corpus]\n",
    "    vocab = Counter()\n",
    "    for w in corpus:\n",
    "        for s in w.split():\n",
    "            vocab[s] += 1\n",
    "    return merges, vocab\n",
    "\n",
    "def apply_merges(word, merges):\n",
    "    s = ' '.join(list(word)) + ' _'\n",
    "    for a, b in merges:\n",
    "        s = s.replace(f\"{a} {b}\", f\"{a}{b}\")\n",
    "    return s.split()\n",
    "\n",
    "merges_en, vocab_en = train_bpe_from_text(paragraph_en, num_merges=30)\n",
    "\n",
    "# Approximate top pairs after training by recomputing pair counts on the merged corpus\n",
    "def top_pairs_after_training(text, merges, topk=5):\n",
    "    words = re.findall(r\"\\w+\", text.lower(), flags=re.UNICODE)\n",
    "    corpus = [' '.join(list(w)) + ' _' for w in words]\n",
    "    for a,b in merges:\n",
    "        pattern = f\"{a} {b}\"\n",
    "        repl = f\"{a}{b}\"\n",
    "        corpus = [w.replace(pattern, repl) for w in corpus]\n",
    "    counts = Counter()\n",
    "    for w in corpus:\n",
    "        syms = w.split()\n",
    "        for i in range(len(syms)-1):\n",
    "            counts[(syms[i], syms[i+1])] += 1\n",
    "    return counts.most_common(topk)\n",
    "\n",
    "# Five longest subword tokens\n",
    "longest_tokens = sorted(vocab_en.keys(), key=len, reverse=True)[:5]\n",
    "\n",
    "print(\"=== Q3.3 BPE on English paragraph ===\")\n",
    "print(\"Paragraph:\", paragraph_en)\n",
    "print(\"\\nFive most frequent pairs after training (approx):\")\n",
    "print(top_pairs_after_training(paragraph_en, merges_en, topk=5))\n",
    "\n",
    "print(\"\\nFive longest subword tokens:\")\n",
    "print(longest_tokens)\n",
    "\n",
    "# Segment 5 words (include a rare and an inflected form)\n",
    "for w in [\"reading\", \"readers\", \"skills\", \"slower\", \"unreadable\"]:\n",
    "    print(w, \"→\", apply_merges(w.lower(), merges_en))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2611603",
   "metadata": {},
   "source": [
    "**Brief reflection (5–8 sentences).**  \n",
    "On this small English paragraph, BPE learned stems like `read` and recurring suffixes like `ing_` and `ers_`. Longer tokens often correspond to frequent subsequences such as entire short words (`is_`, `a_`) or high-frequency bigrams merged repeatedly. Subword tokenization works well for handling derived forms like *readers* and *reading*, and it can also process an unseen word like *unreadable* by combining known pieces (e.g., `un`, `read`, `able_`). Pros: reduces OOVs and captures productive morphology. Cons: merges can split along non-morphemic boundaries and may overfit to the training snippet. For Telugu or other agglutinative languages, BPE can learn common suffixes but might still fragment long, low-frequency words, making interpretability and alignment with true morphology imperfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b967ea",
   "metadata": {},
   "source": [
    "## Q4. Word Pair: Sunday → Saturday\n",
    "\n",
    "We compute distances under two cost models and produce a DP table excerpt for the (Sub=2) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_edit_distance(s1, s2, sub_cost=1, ins_cost=1, del_cost=1):\n",
    "    n, m = len(s1), len(s2)\n",
    "    D = [[0]*(m+1) for _ in range(n+1)]\n",
    "    for i in range(1, n+1):\n",
    "        D[i][0] = D[i-1][0] + del_cost\n",
    "    for j in range(1, m+1):\n",
    "        D[0][j] = D[0][j-1] + ins_cost\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else sub_cost\n",
    "            D[i][j] = min(\n",
    "                D[i-1][j] + del_cost,       # deletion\n",
    "                D[i][j-1] + ins_cost,       # insertion\n",
    "                D[i-1][j-1] + cost          # substitution / match\n",
    "            )\n",
    "    return D\n",
    "\n",
    "s1 = \"SUNDAY\"\n",
    "s2 = \"SATURDAY\"\n",
    "\n",
    "# Model A\n",
    "D_A = min_edit_distance(s1, s2, sub_cost=1, ins_cost=1, del_cost=1)\n",
    "dist_A = D_A[len(s1)][len(s2)]\n",
    "\n",
    "# Model B\n",
    "D_B = min_edit_distance(s1, s2, sub_cost=2, ins_cost=1, del_cost=1)\n",
    "dist_B = D_B[len(s1)][len(s2)]\n",
    "\n",
    "print(\"=== Q4 Distances ===\")\n",
    "print(\"Model A (sub=1, ins=1, del=1):\", dist_A)\n",
    "print(\"Model B (sub=2, ins=1, del=1):\", dist_B)\n",
    "\n",
    "# One valid edit sequence (informal) from Sunday -> Saturday\n",
    "edit_sequence = [\n",
    "    \"SUNDAY\",\n",
    "    \"SU NDAY  -> insert 'A' after 'S' → SAUNDAY\",\n",
    "    \"SAUNDAY  -> insert 'T' after 'SA' → SATUNDAY\",\n",
    "    \"SATUNDAY -> substitute 'N' with 'R' → SATURDAY\",\n",
    "]\n",
    "print(\"\\nOne valid edit sequence:\")\n",
    "for step in edit_sequence:\n",
    "    print(\"-\", step)\n",
    "\n",
    "# DP table excerpt for sub=2: first 3 rows (i=0..3) and first 4 columns (j=0..4)\n",
    "D = D_B  # sub=2\n",
    "labels_rows = [\"\", *s1[:3]]  # \"\", S, U, N\n",
    "labels_cols = [\"\", *s2[:4]]  # \"\", S, A, T, U\n",
    "\n",
    "print(\"\\nDP Table (sub=2) — rows i=0..3 (S1), cols j=0..4 (S2):\")\n",
    "for i in range(0, 4):\n",
    "    row_vals = D[i][:5]\n",
    "    print(f\"i={i:>1} ({labels_rows[i]:>1}) :\", row_vals)\n",
    "\n",
    "print(\"\\nD(3,4) =\", D[3][4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
